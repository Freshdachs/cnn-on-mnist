\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Project 1: Applying Convolutional Neural Networks on MNIST}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  In this short paper, we explore how to use Convolutional Neural Networks for the MNIST challenge. Furthermore, we explore how the design of our network, as well as the choice of hyperparameters affects the performance of our CNN-approach. We will conclude with the proposal of a minimal CNN which represents an optimal tradeoff between complexity and prediction accuracy.
\end{abstract}

\section{Introduction to MNIST and Convolutional Neural Networks}

\section{Implementing a simple CNN for the MNIST challenge}
\subsection{Basic Architecture}
In this section, we will start with a prototypical CNN architecture, consisting of 2 convolutional layers, a convolutional layer and an output layer. The output of every layer (except from the output layer) is activated by a rectified linear unit. Furthermore, the output of our convolutional layers is pooled with windowsize 2 and stepsize 2. 


\subsection{Training Process}
\subsection{Evaluating Test Performance}



\section{Effect of Architecture and Hyperparameters on performance}

\subsection{Preface: On Randomness and Determination in Neural Network training}
Training Neural Networks is usually not a deterministic process. While the optimization algorithms themselves are often deterministic, Initialization of the layer Variables usually follows some random distributions.


\subsection{Effect of Iteration Count}
Optimization of Neural Networks is usually done by some form of gradient descent. Gradient Descent follows a decline in a loss function and thus seeks some form of localized minimum. This means that during the optimization process, our network configuration converges against a local minimum.

Thus, increasing Iteration Count has a beneficial effect on accuracy, as long as we haven't yet converged against our optimal configuration. However, one can observe diminishing returns once the optimization process has reached a loss minimum. Increasing Iteration Count does not allow us to improve the accuracy of our predictions beyond a certain threshold, which is determined by our network architecture and a little bit of chance (since the local maximum we find is dependent on our starting point).
\subsection{Effect of Learning Rate on the Optimization Process}
\subsection{Effect of Pooling}
\subsection{Effect of general network shape on Learning}

\section{Designing a minimal efficient CNN}



\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include
acknowledgments in the anonymized submission, only in the final paper.

\section*{References}

References follow the acknowledgments. Use unnumbered first-level
heading for the references. Any choice of citation style is acceptable
as long as you are consistent. It is permissible to reduce the font
size to \verb+small+ (9 point) when listing the references. {\bf
  Remember that you can use a ninth page as long as it contains
  \emph{only} cited references.}
\medskip

\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
  Exploring Realistic Neural Models with the GEneral NEural SImulation
  System.}  New York: TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
learning and recall at excitatory recurrent synapses and cholinergic
modulation in rat hippocampal region CA3. {\it Journal of
  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
